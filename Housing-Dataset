import pandas as pd
import numpy as  np
from  sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn import model_selection 
from sklearn.model_selection import StratifiedShuffleSplit
from pandas.plotting import scatter_matrix
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ShuffleSplit

data=pd.read_csv("D:\Datasets\HousingData.csv")

data.head()

data["CRIM"].value_counts#describe each district of category

data.info()#used to get quick informatioon of data

data.describe()#show sumaary of attribute

#plot graphs
data.hist(bins=20,figsize=(20,15))
plt.show

#create test dataset
train_set,test_set=model_selection .train_test_split(data,test_size=0.2  ,random_state=42)

#pd.cut() this function is used to create an income category attribute with 5 categories
data['tax_data']=pd.cut(data['TAX'],bins=[0,1.5,3.,4., np.inf],labels=[1,2,3,4])

data['tax_data'].hist()

#
split= StratifiedShuffleSplit(n_splits=1,test_size=0.2 ,random_state=42)
for train_index,test_index in split.split(data,data['tax_data']):
    strat_train_set=data.iloc[train_index]
    strat_test_set=data.iloc[test_index]

strat_test_set["tax_data"].value_counts() / len(strat_test_set)

#now we should remove the data back into its original position
for set_ in (strat_train_set,strat_test_set):
    set_.drop("tax_data",axis =1,inplace =True)

#create a copy
data=strat_train_set.copy()

#correlations
corr_matrix=data.corr()

corr_matrix['AGE'].sort_values(ascending=False)

attributes = ["CRIM", "ZN", "INDUS",
 "CHAS"]
scatter_matrix(data[attributes], figsize=(12, 8))

# Get rid of missing values
imputer = SimpleImputer(strategy="median")
data_num=data.drop('CHAS',axis=1)
imputer.fit(data_num)

imputer.statistics_

data_num.median().values


x=imputer.transform(data_num)

#if you put back into pandas Dtaframe
data_=pd.DataFrame(x, columns=data_num.columns)

data_cat = data[["CHAS"]]
data_cat.head(10)


data.isnull().sum()

data.dropna(inplace=True)
data.isnull().sum()

data['CRIM'].mean()

data['CRIM'].replace(np.NAN,data['CRIM'].mean()).head(10)

data['ZN'].mean()

data['ZN'].replace(np.NAN,data['ZN'].mean()).head(10)

data['INDUS'].mean()

data['INDUS'].replace(np.NAN,data['INDUS'].mean()).head(10)

data['CHAS'].mean()
data['CHAS'].replace(np.NAN,data['CHAS'].mean()).head(10)


data['RM'].mean()
data['RM'].replace(np.NAN,data['RM'].mean()).head(10)

data['NOX'].mean()
data['NOX'].replace(np.NAN,data['NOX'].mean()).head(10)

data['AGE'].mean()
data['AGE'].replace(np.NAN,data['AGE'].mean()).head(10)

data['DIS'].mean()
data['DIS'].replace(np.NAN,data['DIS'].mean()).head(10)

data['RAD'].mean()
data['RAD'].replace(np.NAN,data['RAD'].mean()).head(10)

data['TAX'].mean()
data['TAX'].replace(np.NAN,data['TAX'].mean()).head(10)

data['PTRATIO'].mean()
data['PTRATIO'].replace(np.NAN,data['PTRATIO'].mean()).head(10)

data['B'].mean()
data['B'].replace(np.NAN,data['B'].mean()).head(10)

data['LSTAT'].mean()
data['LSTAT'].replace(np.NAN,data['LSTAT'].mean()).head(10)

data['MEDV'].mean()
data['MEDV'].replace(np.NAN,data['MEDV'].mean()).head(10)


num_pipeline = Pipeline([
 ('imputer', SimpleImputer(strategy="median")),

 ('std_scaler', StandardScaler()),
 ])
data_num = num_pipeline.fit_transform(data_num)



data_num


# Import 'train_test_split'
# my target is MEDV for training and testing data
prices = data['MEDV']
features = data.drop('MEDV', axis = 1)
from sklearn.model_selection import train_test_split

# Shuffle and split the data into training and testing subsets
X_train, X_test, y_train, y_test = train_test_split(features,prices ,test_size=0.2, random_state = 42)

# Success
print("Training and testing split was successful.")







def fit_model(x,y):
    cv_sets=ShuffleSplit(n_splits=10,test_size=0.2,random_state=0)
    # Create a decision tree regressor object
    regressor=DecisionTreeRegressor()
    # Create a dictionary for the parameter 'max_depth' with a range from 1 to 10
    params={'max_depth':[1,2,3,4,5,6,7,8,9,10]}
    # Create the grid search cv object --> GridSearchCV()
    grid=GridSearchCV(estimator=regressor,param_grid=params,cv=cv_sets)
    
# Fit the grid search object to the data to compute the optimal model
    grid.fit(x,y)
# Return the optimal model after fitting the data
    return grid.best_estimator_

reg = fit_model(X_train, y_train)
# produce the value of 'max_depth'
print("parameter 'max_depth'is {} for optimal model.".format(reg.get_params()['max_depth']))







